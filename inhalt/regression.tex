\section{Linear Regression}
\subsection*{OLS}
Estimator: $\hat{\mathbf{\beta}} = (\mathbf{X}^{\top}\mathbf{X})^{-1}\mathbf{X}^{\top}\mathbf{y}$\\
$E(x_{ti}u_t) = 0 \implies \mathbf{X}^{\top}(\mathbf{y}-\mathbf{X}\mathbf{\beta}) = \mathbf{0}$\\
SSR($\beta$): $= \sum_{t=1}^{n}(y_i - \mathbf{X_t}\mathbf{\beta})^2$\\
$\mathbf{y}^{\top} \mathbf{y} = \hat{\beta}^{\top} \mathbf{X}^{\top}\mathbf{X}\hat{\beta} + (\mathbf{y} - \mathbf{X}\hat{\beta} )^{\top} (\mathbf{y} - \mathbf{X}\hat{\beta})$\\
Projection: $\mathbf{P_X = X(X^{\top} X)^{-1} X^{\top}}$\\
$\mathbf{M_X = I- P_X = I - X(X^{\top} X)^{-1} X^{\top}}$\\
$\mathbf{P_x y = X((X^{\top}X)^{-1}X^{\top}y)}$; $\mathbf{P_X P_X = P_X}$\\
$\mathbf{M_X y = \hat{u}}$; $\mathbf{M_X X = 0}$; $\mathbf{M_X M_X = M_X}$\\
$\mathbf{P_X + M_X = I}$; $\mathbf{P_X M_X} = \mathbf{0}$; $\mathbf{P_X ^{\top} = P_X}$\\
$\lVert \mathbf{y} \rVert ^2 = \lVert \mathbf{P_X y} \rVert ^2 + \lVert \mathbf{M_X y} \rVert ^2$; $\lVert \mathbf{P_X y \rVert \leq \lVert \mathbf{y} \rVert}$\\
Centering: $\mathbf{M_{\iota}x = z = x} - \bar{x}\mathbf{\iota}$; $\mathbf{\iota^{\top} z} = 0$
$\mathbf{P_1 \equiv P_{X_1}}$; $\mathbf{P_1 P_X = P_X P_1 = P_1}$\\
FWL: $\beta_2$ from $\mathbf{y = X_1\beta_1 +X_2 \beta_2 + u}$ and $\mathbf{M_1 y = M_1 X_2 \beta_2} + \text{res}$  are the same. (+ res)
Seasonal w const: $\mathbf{s}_i ^{\prime} = \mathbf{s}_i - \mathbf{s_4}$, $i=1,2,3$.
Avg is const coeff.
$\mathbf{M_{S}y}$ is deseasonalized.
$\beta^{(t)}-\hat{\beta} = -1 \backslash 1-h_t \mathbf{(X^{\top}X)^{-1}X_t ^{\top}}\hat{u_t}$ where $h_t$ denotes the t\textsuperscript{th} diagonal element of $\mathbf{P_X}$.
\subsection*{Bias}
vector of (true) model params: $\theta$\\
Bias: $E(\hat{\theta}) - \theta_0$, $E(\mathbf{(X^{\top}X)^{-1}X^{\top}u}) = 0$\\
estimating eq: $g(\mathbf{y},\mathbf{\theta}) = 0$ unbiased iff $\forall \mu \in \mathbb{M}, E_{\mu}g(\mathbf{y,\theta_\mu}) = \mathbf{0}$ or $E(\mathbf{X^{\top}u) = 0}$\\
$X$ exogenous $\implies$ $E(\mathbf{u \mid X) = 0}$ and both  $\hat{\beta}$ and estimating equations unbiased.\\
Make exog assumpt in cross-sec not time.\\
regressors predetermined: $E(\mathbf{X^{\top}u}) = \mathbf{0}$
\subsection*{Stochastic Limits}
Converg in prob: $\lim Pr(\lvert Y_n - Y_{\infty} \rvert > \epsilon) = 0 \Rightarrow \text{p}\lim Y_n = Y_{\infty} \implies$ converg dist\\
Converg dist: $\lim F_n (x) = F(x) \equiv Y_n \rightarrow F$\\
LLN: $\text{plim} \bar{Y_n} = \text{plim} \frac{1}{n} \sum^{n}_{t} Y_t = \mu_{Y}$, $Y_t$ IID, $\bar{Y_n}$ sample mean of $Y_t$, $\mu_T$ pop mean.
LLN2: $\text{plim} \frac{1}{n} \sum_{t}^{n} Y_t = \lim \frac{1}{n} \sum_{t}^{n} E(Y_t)$\\
$\text{plim} Y_n Z_n = \text{plim} Y_n \text{plim} Z_n$ if converg\\
$\mathbf{X^{\top}X}$ may not have plim so mult by $1 \backslash n$.\\
consistent: $\text{plim}_{\mu} \hat{\beta} = \beta_\mu$, may be bias\\
$E(\mu_t \mid \mathbf{X}_t) = 0 \implies \hat{\beta}$ consistent.
\subsection*{Covariance and Precision Matrices}
$\text{Cov}(b_i, b_j) \equiv E((b_i - E(b_i))(b_j-E(b_j)))$\\
if $i=j$, $\text{Cov}(b_i, b_j) = \text{Var}(b_i)$\\
$\text{Var}(\mathbf{b}) \equiv E((\mathbf{b}-E(\mathbf{b}))(\mathbf{b}-E(\mathbf{b}))^{\top})$\\
when $E(\mathbf{b}) = \mathbf{0}$, $\text{Var}(\mathbf{b}) = E(\mathbf{bb^{\top}})$
$b_i$, $b_j$ indep: $\text{Cov}(b_i, b_j) = 0$, converse false\\
correlation: $\rho(b_i, b_j) \equiv \frac{\text{Cov}(b_i, b_j)}{(\text{Var}(b_i)\text{Var}(b_j))^{1 \backslash 2}}$
$\text{Var}(\mathbf{b})$ positive semidefinite.
cov and corr matrix positive definite most of the time.\\
positive definite: $\mathbf{x^{\top}Ax} > 0$ for $\mathbf{x} \in k \times 1$.\\
$\mathbf{x^{\top}Ax} = \sum_{i}^{k}\sum_{j}^{k} x_i x_j A_{ij}$.
If $ \geq 0 \Rightarrow$ semidef.\\
Any $\mathbf{B^{\top}B}$ is pos semidef.
If full col rank then pos def.
pos def $\Rightarrow$ diag $> 0$ \& non-singular.
(pos def)$^{-1}$ $\exists$ \& is pos def.\\
Precision mtrx: invers of cov mtrx of estmatr.
$\exists$ \& pos def iff cov mtrx pos def.\\
If $u$ IID w Var $\sigma^2$ and cov of any pair $=0$: $\text{Var}(\mathbf{u})=E(\mathbf{uu^{\top}}) = \sigma^{2}\mathbf{I}$.
If false, $\mathbf{\Omega} = $ err cov mtrx.
If diag of $\mathbf{\Omega}$ differ, heteroskedastic.
Homoskedastic: all $u$ same Var.
Autocorrelated: off-diag $\mathbf{\Omega} \neq 0$.\\
$\hat{\beta}$ unbiased \& $\mathbf{\Omega}=\sigma^{2}\mathbf{I}$ so no hetero or autocorr, then $\text{Var}(\hat{\beta})=\sigma^2(\mathbf{X^{\top}X})^{-1}$.\\
Precision affected by $n$, $\sigma^2$, $X$.\\
Collinearity: precision for $\beta_1$ dep on $\mathbf{X_2}$.
\subsection*{Efficiency}
$\tilde{\beta}$ more efficient than $\hat{\beta}$ iff $\text{Var}(\tilde{\beta})^{-1} - \text{Var}(\hat{\beta})^{-1}$ is nonzero pos semidef mtrx.\\
Gauss-Markov: If $E(\mathbf{u} \mid \mathbf{X}) = \mathbf{0}$ and $E(\mathbf{uu^{\top}} \mid \mathbf{X}) = \sigma^{2}\mathbf{I}$ then OLS $\hat{\beta}$ is BLUE ( best linear unbiased estimator ).
Not necessary that $u$ normally distributed.
\subsection*{Residuals \& Disturbances}
$\mathbf{\hat{u}} = \mathbf{M_X u}$ (hat resid, $u$ dist).\\
If $E(\mathbf{u}\mid \mathbf{X}) = \mathbf{0} \Rightarrow E(\lVert \mathbf{\hat{u}} \rVert ^2) \leq E(\lVert \mathbf{u} \rVert ^2)$\\
$\text{Var}(\hat{u_t}) < \sigma^2$; $\hat{\sigma}^2 \equiv \frac{1}{n} \sum_{t}^{n} \hat{u}_t ^2$\\
$E(\hat{\sigma}^2) = \frac{n-k}{n}\sigma^2$\\
$E(\mathbf{u^{\top}M_X u}) = E(\text{SSR}(\hat{\beta})) = (n-k)\sigma^2$\\
unbiased: $s^2 \equiv \frac{1}{n-k}\sum_{t}^{n} \hat{u}_t ^2$; $s=$ std err.\\
unbias est of $\text{Var}(\hat{\beta})$: $\widehat{\text{Var}}(\hat{\beta})=s^2(\mathbf{X^{\top}X})^{-1}$\\
$s^2$ unbiased and consistent.\\
$\text{MSE}(\tilde{\beta}) \equiv E((\tilde{\beta} - \beta_0)(\tilde{\beta} - \beta_0)^{\top})$\\
I $\tilde{\beta}$ unbiased $\text{MSE}(\tilde{\beta}) = \text{Var}(\tilde{\beta})$.
\subsection*{Measures of Goodness of Fit}
$R_{u}^2 = \frac{\text{ESS}}{\text{TSS}}= \frac{\lVert \mathbf{P_X y \rVert^2}}{\lVert \mathbf{y} \rVert^2} = \cos^2 \theta$, where $\theta$ angle between $\mathbf{y}$ and $\mathbf{P_X y}$.
$0 \leq R^2_{u} \leq 1$.\\
$R_{c}^2$: center all vars first.
Invalid if $\iota \notin \mathcal{S}(\mathbf{X})$.\\
$R_{c}^2 = 1 - \sum_{t}^{n} \hat{u}_t ^2 \; \backslash \; \sum_{t}^{n} (y_t - \bar{y})^2$.\\
Adj $R^2$: unbiased estimators.
maybe $< 0$.\\
$\bar{R}^2 \equiv 1- \frac{\frac{1}{n-k}\sum_{t}^{n} \hat{u_t}^2}{\frac{1}{n-1}\sum_{t}^{n}(y_t - \bar{y})^2} = 1 - \frac{(n-1)\mathbf{y^{\top}M_X y}}{(n-k)\mathbf{y^{\top}M_\iota y}}$\\
$\bar{R}^2$ does not always $\uparrow$ in regressors.
\subsection*{Hypothesis Testing}
If $u_t$ normal, and $\sigma$ known, test $\beta=\beta_0$ w
$z = \frac{\hat{\beta} - \beta_0}{(\text{Var}(\hat{\beta}))^{1 \backslash 2}} = \frac{n^{1 \backslash 2}}{\sigma}(\hat{\beta} - \beta_0)$, $z \sim N(0,1)$\\
NCP: $\lambda = \frac{n^{1 \backslash 2}}{\sigma} (\beta_1 - \beta_0)$, $\beta_1 \neq \beta_0$\\
Reject null if $z$ large enough.
2-tail: $\lvert z \rvert$.\\
Type 1: reject true null, 2: accept false\\
left-tail $\Phi(-c_{\alpha}) = \alpha \backslash 2$, $c_\alpha = \Phi^{-1} (\alpha \backslash 2)$.\\
$\Phi^{-1}(.975) = 1.96$.
Power: prob test rejects the null.
Prob of Type 2 $= 1 - P$(power).
Power $\uparrow$ with $(\beta_1 - \beta_0) \uparrow$ or $\sigma \downarrow$ or $n \uparrow$.\\
$p(z) = 2(1-\Phi(\lvert z \rvert))$\\
$x \sim N(\mu, \sigma^2) \Rightarrow z = (x-\mu) \backslash \sigma$, $z \sim N(0,1)$.\\
Lin comb of rand vars that are jointly multivariate normal must be $\sim N$.
If $\mathbf{x}$ multivar norm with 0 cov, componenets of $\mathbf{x}$ are mutually indep.\\
$\chi^2$: $y \equiv \lVert \mathbf{z} \rVert^2 = \mathbf{z^{\top}z} = \sum_{i}^{m} z_i ^2$., $y \sim \chi^2(m)$
with $\mathbf{z} \sim N(\mathbf{0,I})$; $E(y) = m$.
$\text{Var}(y) = 2m$.\\
$y_1 \sim \chi^2(m_1)$ \& $y_2 \sim \chi^2(m_2)$ indep $\Rightarrow y_1 + y_2 \sim \chi^2 (m_1 + m_2)$\\
$m\times1$ $\mathbf{x} \sim N(\mathbf{0,\Omega})$, then $\mathbf{x^{\top}\Omega^{-1}x} \sim \chi^2(m)$\\
If $\mathbf{P}$ $n \times n$ w rank $r < n$ and $\mathbf{z} \sim N(\mathbf{0,I})$ then $\mathbf{z^{\top}Pz} \sim \chi^2(r)$.\\
$z \sim N(0,1)$ \& $y \sim \chi^2(m)$, $z,y$ indep, then $t \equiv z \setminus (y \setminus m)^{1 \setminus 2}$.
Or $t \sim t(m)$.
Only first $m-1$ moments exist.
Cauchy: $t(1)$.
$\text{Var}(t) = m \setminus (m-2)$.
$t(m)$ tends to std norm.\\
$y_1$, $y_2$ indep rand var $\sim \chi^2 (m_1)$ \& $\chi^2(m_2)$, then $F \equiv \frac{y_1 \setminus m_1}{y_2 \setminus m_2}$.
$F \sim F(m_1, m_2)$.
As $m_2 \rightarrow \infty$, $F \sim 1 \setminus m_1$ times $\chi^2 (m_1)$.
$t \sim t(m_2) \Rightarrow t^2 \sim F(1,m_2)$.\\
\subsection*{Exact Tests ($\mathbf{u} \sim N(\mathbf{0}, \sigma^2 \mathbf{I})$)}
$\frac{\mathbf{x_2^{\top} M_1 y}}{s(\mathbf{x_2 ^{\top} M_1 x_2})^{1 / 2}} = \left( \frac{\mathbf{y^{\top}M_X y }}{n-k} \right)^{-\frac{1}{2}} \frac{\mathbf{x_2^{\top} M_1 y}}{(\mathbf{x_2 ^{\top} M_1 x_2})^{1 \backslash 2}}$\\
is t-stat $t_{\beta_2} \sim t(n-k)$ for testing $\beta_2 = 0$.\\
$\beta_2 \in \mathbb{R} \Rightarrow$ test for $\beta_2 = \beta_{20}$: $(\hat{\beta_2} - \beta_{20}) / s_2$.
$F_{\beta_2} \equiv \frac{(\text{RSSR - USSR}) / r}{\text{USSR} / (n-k) } = \frac{\mathbf{y^{\top}P_{M_1 X_2} y}/r}{\mathbf{y^{\top}M_X y}/(n-k)}$ is F-stat $\sim F(r,n-k)$, used for multiple hyp on $\mathbf{\beta_2}$.
Under null, $\mathbf{M_1 y = M_1 u} \Rightarrow F_{\beta_2} = \frac{\mathbf{\epsilon^{\top}P_{M_1 X_2} \epsilon /}r }{\mathbf{\epsilon^{\top}M_X \epsilon}/(n-k)}$, where $\mathbf{\epsilon \equiv u} / \sigma$, $\mathbf{P_{M_1 X_2} = P_x - P_1}$.
P-value for F is $1-F_{r,n-k}(F_{\beta_2})$.
When only 1 restriction, F and 2-tailed t test are the same.
If testing all $\beta = 0$, $F = \frac{n-k}{k-1} \times \frac{R_c ^2}{1-R_c ^2}$.
If testing $\beta_1 = \beta_2$, let $\gamma = \beta_2 - \beta_1$ then $F_{\gamma} = \frac{(\text{RSSR} - \text{SSR}_1 - \text{SSR}_2) / k}{(\text{SSR}_1 + \text{SSR}_2) / (n-2k)}$
\subsection*{Asymptotic Theory}
EDF: $\hat{F}(x) \equiv \frac{1}{n}\sum_{t}^{n} \mathbb{I}(x_t \leq x)$.
FTS: $\text{plim} \hat{F}(x) = F(x)$.
CLT: $z_n \equiv \frac{1}{\sqrt{n}}\sum_{t}^{n} \frac{x_t - \mu}{\sigma}$ asymptotically $\sim N(0,1)$ if $x_t$ IID.
Uncorrelated $x_t$ with $E(x_t) = 0 \Rightarrow n^{-1/2} \sum_{t}^{n} x_t$ goes to $N(0, \lim \frac{1}{n} \sum_{t}^{n} \text{Var}(x_t))$.
If $\mathbf{u} \sim IID(\mathbf{0}, \sigma^2 \mathbf{I})$, $E(u_t \mid \mathbf{X}_t) = 0$, $E(u_t ^2 \mid \mathbf{X_t} = \sigma^2)$, $\text{plim}\frac{1}{n}\mathbf{X^{\top}X} = S_{X^{\top}X}$ where $S$ finite, deterministic, pos def mtrx, then $n^{1/2}(\mathbf{\hat{\beta} - \beta_0}) \xrightarrow{d} N(\mathbf{0}, \sigma^2 \mathbf{S}^{-1}_{\mathbf{X^{\top}X}})$ and $\text{plim}\;s^2 (n^{-1}\mathbf{X^{\top}X})^{-1} = \sigma^2 \mathbf{S}^{-1}_{\mathbf{X^{\top}X}}$.\\
An estimator for cov mtrx is consistent if $\text{plim} (n \widehat{\text{Var}(\mathbf{\hat{\theta}})}) = \mathbf{V(\theta)}$, where $\mathbf{V(\theta)}$ is limiting cov mtrx of $n^{1/2}(\mathbf{\hat{\theta} - \theta_0})$\\
If $u$ IID and testing $\beta_2 = \beta_2 ^0$, $t_{\beta_2} = \frac{\hat{\beta_2}-\beta_2^{0}}{\sqrt {s^2 (\mathbf{X^{\top}X})^{-1}_{22}}}$ and $t_{\beta_2} \stackrel{a}{\sim} N(0,1) \Rightarrow t_{\beta_2} = O_p (1)$.
Under null $\mathbf{\beta_2} = \mathbf{0}$, w predetermined regressors $rF_{\beta_2} \stackrel{a}{\sim} \chi^2 (r) $ where $r = k_2$ is dim of $\mathbf{\beta_2}$.\\
$W(\hat{\beta}) = (\mathbf{R}\hat{\beta} - r)^{\top}(\mathbf{R}\widehat{\text{Var}}(\hat{\beta})\mathbf{R}^{\top})^{-1}(\mathbf{R}\hat{\beta} - r)$ is Wald where cov mtrx consistent.
$W(\hat{\beta}) \stackrel{a}{\sim} \chi^2 (r)$ under null where $r$ is r-vector.
\subsection*{Multiple Testing}
FWER: $\alpha_m = 1 - (1-\alpha)^m$.\\
Bonferroni: $Pr(\cup_{i}^{m} P_i \leq \alpha / m) \leq \alpha$.\\
Simes: $P_{(j)} \leq j \alpha / m$ for increasing $P$.
Bonf more conservative than Simes.
\subsection*{Power}
If $\mathbf{z} \sim N(\mathbf{\mu, I})$ then $\mathbf{z^{\top}z} \sim$ non-central $\chi^2(m,\Lambda = \mathbf{\mu^{\top}\mu})$ If $\mathbf{z} \sim N(\mathbf{\mu, I})$ then $\mathbf{z^{\top}z} \sim$ non-central $\chi^2(m,\Lambda = \mathbf{\mu^{\top}\mu})$.
Under null $\beta_2 = \mathbf{0} \Rightarrow \Lambda = 0$.\\
$t(n-k,\lambda) \sim \frac{N(\lambda, 1)}{(\chi^2(n-k)/(n-k))^{1/2}}$, $\lambda^2 = \Lambda$.
Pretest estimator: $\acute{\beta} = \mathbb{I}(F_{\gamma = 0} > c_\alpha)\hat{\beta} + \mathbb{I}(F_{\gamma=0} \leq c_\alpha)\tilde{\beta}$ where $c_{\alpha}$ is critical value for F test with $r$ and $n-k-r$ df at $\alpha$.
$\acute{\beta} = \hat{\beta}$ when pretest rejects and $\acute{\beta} = \tilde{\beta}$ when reject.
\subsection*{Confidence \& Sandwich Cov Matrices}
$\theta_0 \in $ confidence set iff $\tau(\mathbf{y}, \theta_0) \leq c_{\alpha}$, if $\theta_0$ true then prob is $1-\alpha$.
Asymp t-stat: $(\hat{\theta}-\theta)/s_{\theta}$.
Pivot: same distribution $\forall \text{DGP}$.
CI exact only if $\tau$ pivot.
If no stat with known finite sample dist, use Wald with $k_2$ vector $\hat{\theta}_2$ asym normal: $(\hat{\theta}_2 - \theta_{20})^{\top}(\widehat{\text{Var}}(\hat{\theta}_2))^{-1}(\hat{\theta}_2 - \theta_{20}) \leq c_{\alpha}$.
$\text{Var}(\hat{\beta}) = (\mathbf{X^{\top}X})^{-1} \mathbf{X}^{\top}\mathbf{\Omega}\mathbf{X(X^{\top}X)^{-1}}$\\
$\hat{\mathbf{\Omega}}$: $\frac{1}{n} \sum_{t}^{n} \hat{u}_t ^2 x_{ti} x_{tj}$\\
$\widehat{\text{Var}}(\hat{\beta}) = (\mathbf{X^{\top}X})^{-1} \mathbf{X}^{\top}\mathbf{\hat{\Omega}}\mathbf{X(X^{\top}X)^{-1}}$\\
$HC_0$: Use $\hat{u}_t ^2$ in diag of $\mathbf{\hat{\Omega}}$ and 0 else.\\
$HC_1$: Use $\hat{u}_t ^2$ in $\mathbf{\hat{\Omega}}$ then mult by $n / (n-k)$\\
$HC_2$: $\hat{u}_t ^2 / (1-h_t)$ with $h_t \equiv \mathbf{X_t(X^{\top}X)^{-1}X_t ^{\top}}$.\\
$HC_3$: $\hat{u}_t ^2 / (1-h_t)$ jackknife, for big varianc.\\
Ignore hetero for std err of sample mean.
HAC for when $u_t$ hetero and/or autocorr.
$\mathbf{\hat{\Sigma}} = (1 / n) \mathbf{X^{\top}\hat{\Omega}X}$ (Newey-West/H. White)\\
$f(a+h) = f(a) + hf^{\prime}(a+\lambda h)$, $h=b-a$, $0<\lambda <1$.
\subsection*{Bootstrap}
$\hat{p}^{\ast}(\hat{\tau}) = \frac{1}{B} \sum_{b}^{B} \mathbb{I}(\tau_{b}^{\ast} > \hat{\tau})$\\
$\hat{p}^{\ast}_{et}(\hat{\tau}) = 2 \min \left( \frac{1}{B} \sum_{b}^{B} \mathbb{I} (\tau_{b}^{\ast} \leq \hat{\tau}), \frac{1}{B} \sum_{b}^{B} \mathbb{I} (\tau_{b}^{\ast} > \hat{\tau}) \right)$ when biased param est and 2-tailed.
$\hat{p}^{\ast}(\hat{\tau}) \rightarrow p(\hat{\tau})$ as $B \rightarrow \infty$. $\tau$ pivotal.
\subsection*{Instrumental Variables}
Can define $E(u_t \mid \Omega_t) = 0$.\\
Err in variables: indep vars in regr model measured with err.
$E(u_t \mid x_t) \neq 0$, $\text{Cov}(x_t. u_t) \neq 0$.
OLS est biased and inconsist.
Simultaneity: two or more endog vars jointly determined by sys of simultaneous eq.\\
Assume, $E(\mathbf{uu^{\top}}) = \sigma^2 \mathbf{I}$ and at least one in $\mathbf{X}$ not predetermined wrt disturb.
$n \times k$ mtrx $\mathbf{W}$ with $\mathbf{W_t \in \Omega_t}$.
Col of $\mathbf{W}$ are IV.\\
$E(u_t \mid \mathbf{W}_t) = 0$, $\mathbf{W}^{\top}(\mathbf{y - X\beta}) = \mathbf{0}$ are unbiased est eq.
$\hat{\beta}_{IV} \equiv (\mathbf{W^{\top}X})^{-1}\mathbf{W}^{\top}\mathbf{y}$.
$\mathbf{W^{\top}X}$ must be non-sing.
$\hat{\beta}_{IV}$ generally biased but consistent.
Assume $\mathbf{S_{W^{\top}X}} \equiv \text{plim} \frac{1}{n}\mathbf{W^{\top} X}$ is deterministic and non-sing.
Same for $\mathbf{S_{W^{\top}W}}$.
$\hat{\beta_{IV}}$ consistent iff $\text{plim} \frac{1}{n} \mathbf{W^{^\top}u} = 0$.
Asym cov mtrx of IV est: $\sigma_{0}^{2} \text{plim}(n^{-1}\mathbf{X^{\top}P_W X})^{-1}$.
$\mathbf{J}$: full col rank, asym deterministic, min asym cov mtrx of IV est.\\
GIVE: $\hat{\beta}_{IV} = (\mathbf{X^{\top}P_W X})^{-1} \mathbf{X^{\top} P_W y}$
IV est: Col of $P_W X$ should be lin indep.\\
IV asym normal like all est.\\
$\widehat{\text{Var}}(\hat{\beta}_{IV}) = \hat{\sigma}^2 (\mathbf{X^{\top}P_W X})^{-1}$.\\
$\hat{\sigma}^2 = \lVert \mathbf{y - X \hat{\beta_{IV}}} \rVert ^{2} / n$.
\subsection*{Generalized Least Squares}
Consider $E(\mathbf{uu^{\top}}) = \mathbf{\Omega}$, $\mathbf{\Omega}^{-1} = \mathbf{\Psi \Psi^{\top}}$\\
$\hat{\beta}_{GLS} = (\mathbf{X^{\top} \Omega^{-1} X})^{-1} \mathbf{X^{\top} \Omega^{-1} y}$\\
$E(\mathbf{\Psi^{\top} uu^{\top} \Psi}) = \mathbf{I}$.\\
$\text{Var}(\hat{\mathbf{\beta}}^{\top}_{GLS}) = (\mathbf{X^{\top} \Omega^{-1}X})^{-1}$
autocovariance of AR(1): $\mathbf{\Omega}(p) = \frac{\sigma_{\epsilon}^2}{1-\rho^2} \times$ mtrx with 1 diag and $\rho^i$ increasing away from diag.
$\text{Cov}(u_t, u_{t-1}) = \rho \sigma_u ^2$.\\
$\sigma_u ^2 = \equiv \sigma_{\epsilon}^2 / (1-\rho^2)$\\
$u_t = \rho u_{t-1} + \epsilon_t$, $\epsilon_t \sim IID(0, \sigma_{\epsilon}^2)$

